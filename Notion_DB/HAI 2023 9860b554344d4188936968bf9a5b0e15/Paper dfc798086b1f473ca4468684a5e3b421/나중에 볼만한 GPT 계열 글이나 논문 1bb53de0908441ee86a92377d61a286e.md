# 나중에 볼만한 GPT 계열 글이나 논문

## 모델 구조

[ALiBi](https://arxiv.org/abs/2108.12409): Train Short, Test Long

- positional encoding
- sinusoidal보다 빠르고 적은 메모리
- 짧게 학습하고 길게 생성이 가능

[RETRO](https://www.deepmind.com/publications/improving-language-models-by-retrieving-from-trillions-of-tokens): Improving language models by retrieving from trillions of tokens

- BERT+knn을 통해서 DB의 인접한 토큰을 가져와 참고함으로써 성능 향상
- [Mengzi-Retrieval-LM](https://github.com/Langboat/mengzi-retrieval-lm)이 비슷한 구조로 참고할만함

[Flamingo](https://deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model): Tackling multiple tasks with a single visual language model

- vision encoder, lm을 frozen 시키고 추가 파라미터를 학습
- 이미지를 볼 수 있는 GPT, few-shot도 가능
- 오픈소스 구현체 [open_flamingo](https://github.com/mlfoundations/open_flamingo)

## 학습 데이터

[chinchilla](https://www.deepmind.com/publications/an-empirical-analysis-of-compute-optimal-large-language-model-training): An empirical analysis of compute-optimal large language model training

- 모델 크기가 커짐에 따라 학습 데이터도 많아져야 함→GPT-3은 너무 적은 데이터로 학습
- llama가 많은 데이터 사용한 이유중 하나

[HyperClova](https://arxiv.org/abs/2204.13509): On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model

- 학습 데이터에 따른 성능 차이
- 간단하게 지나갈려면 [블로그 글](https://engineering.clova.ai/posts/2022/05/hyperclova-corpus)도 존재

[BLOOM](https://arxiv.org/abs/2211.05100): A 176B-Parameter Open-Access Multilingual Language Model

- 메인은 아니지만 학습 데이터가 다국어+코드 섞음

## 학습 방법

### 강화학습

- [Sparrow](https://arxiv.org/abs/2209.14375): Improving alignment of dialogue agents via targeted human judgements
- [None](https://arxiv.org/abs/2210.10760) : Scaling Laws for Reward Model Overoptimization
- [None](https://arxiv.org/abs/2204.05862): Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback
- HF RLHF 보면 참고할만한 논문 존재

## 기타

[HyperClova](https://arxiv.org/abs/2109.04650): What Changes Can Large-scale Language Models Bring?

- 한국어 토큰화 방법으로 형태소 분석 넣음→성능 향상(Table 6)
- 발표 가면 여기에는 없는 이야기들 더 많음